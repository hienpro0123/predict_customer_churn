{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b689556-233d-407d-8552-ed3c9637cf21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.table(\"workspace.default.customer_churn_dataset_training_master\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea8a226-19b3-4134-8d61-ae7541220b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"Số dòng:\", df.count())\n",
    "print(\"Số cột:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ead52f-615d-482b-ac55-94087e4e3d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "363366c6-499a-4dd9-9aba-9b18ab5221f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ce72ff-65ca-4b99-bf18-0ae9e78d9a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "missing_df = df.select([\n",
    "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "display(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f701f193-c403-4a19-8681-bb9429ee55bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1119cb39-f9ef-4d27-a7b1-bcf69379d839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "for c in df.columns:\n",
    "    print(c, df.select(c).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65fc17e0-af20-45b2-aab3-95c5828eddc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "churn_pd = df.select(\"Churn\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.countplot(x='Churn', data=churn_pd)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f'{int(p.get_height())}',\n",
    "        (p.get_x() + p.get_width()/2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='bottom'\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97af90dc-1e6d-4ce9-bd61-6b79a0096be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# lấy các cột categorical\n",
    "categorical_cols = [f.name for f in df.schema.fields \n",
    "                    if f.dataType.simpleString() == 'string']\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "# indexer\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_index\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "# one hot encoder\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\")\n",
    "    for c in categorical_cols\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "df_processed = pipeline.fit(df).transform(df)\n",
    "\n",
    "display(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1268dd03-a653-4dc1-8b25-fbee55ec5e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Train and test data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b02afdb-31cd-4c2e-99a2-99dad1e190b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e203c962-e2ff-47f1-9bdd-b5b7616b6d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59f951f-e23d-42d8-8af5-eb252ce31821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "string_cols = [f.name for f in df_processed.schema.fields \n",
    "               if f.dataType.simpleString() == \"string\"]\n",
    "\n",
    "print(string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73eca1a8-7b7d-4a11-92ed-0ab5d69a4ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid=\"keep\")\n",
    "    for c in string_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e43525-1e5f-492f-9932-a01f67cf0165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=c+\"_index\", outputCol=c+\"_vec\")\n",
    "    for c in string_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ef305d-11eb-42fa-a023-f77b6a9fa007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols = [f.name for f in df_processed.schema.fields\n",
    "                if f.dataType.simpleString() != \"string\"\n",
    "                and f.name != \"Churn\"]\n",
    "\n",
    "feature_cols = numeric_cols + [c+\"_vec\" for c in string_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cc0aa3-60c2-40dd-a494-daa8fc1c86e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1aa3f61-6a65-4188-946b-04ea63b675e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [c for c in df_processed.columns \n",
    "                if c.endswith(\"_index\") or c.endswith(\"_vec\")]\n",
    "\n",
    "df_clean = df_processed.drop(*cols_to_drop)\n",
    "\n",
    "df_final = pipeline.fit(df_clean).transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb876c9-eed2-4ace-8835-443658ecce72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8bb017-c312-4c1b-b82b-87d9133d0a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\"),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(labelCol=\"Churn\", featuresCol=\"features\"),\n",
    "    \"Random Forest\": RandomForestClassifier(labelCol=\"Churn\", featuresCol=\"features\"),\n",
    "    \"Gradient Boosted Trees\": GBTClassifier(labelCol=\"Churn\", featuresCol=\"features\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ea49564-8816-461c-bc13-2012df8eb2b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Set UC temp path\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/main/default/mlflow_volume/tmp\"\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Churn\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        \n",
    "        fitted_model = model.fit(train_df)\n",
    "        predictions = fitted_model.transform(test_df)\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        \n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        \n",
    "        mlflow.spark.log_model(fitted_model, \"model\")\n",
    "        \n",
    "        print(f\"{model_name} - AUC: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Churn_processing_TrainingSet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
